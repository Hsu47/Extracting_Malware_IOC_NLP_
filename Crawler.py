# from selenium import webdriver
# from selenium.webdriver.support.wait import WebDriverWait
# from selenium.webdriver.support import expected_conditions as EC
# from selenium.webdriver.common.by import By
# from selenium import webdriver
# from selenium.webdriver.chrome.options import Options
import pandas as pd
import requests
from bs4 import BeautifulSoup
import re
import datetime
import os
import nltk
from datetime import datetime
from ArticleClean import utils_preprocess_text

def Crawler():
    print('[INFO]Crawl Malwarebytes Start')
    lst_stopwords = nltk.corpus.stopwords.words("english")
    lst_stopwords.append('network')
    lst_stopwords.append('system')
    lst_stopwords.append('systems')
    lst_stopwords.append('affected')
    lst_stopwords.append('cisa')
    lst_stopwords.append('actor')
    lst_stopwords.append('could')

    ## 動態抓取網頁
    # path = os.getcwd()
    # path += '\chromedriver.exe'
    # chrome_options = Options()
    headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'}
    # chrome_options.add_argument('--headless')
    # chrome_options.add_argument('--disable-gpu')
    # driver = webdriver.Chrome(options=chrome_options, executable_path=path)
    # driver.get("https://blog.malwarebytes.com/category/threat-analysis/")
    ## 基礎變數設置
    UrlList = []
    Title = []
    articleList = []

    ##取得每一篇報導的url
    # for i in range(60):
    #     button = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.LINK_TEXT, 'LOAD MORE')))
    #     button.click()
    Page = 1
    url = 'https://blog.malwarebytes.com/category/threat-analysis/page/'
    while(True):
        pageUrl = url+str(Page)
        r = requests.get(pageUrl,headers = headers, allow_redirects=True)
        soup = BeautifulSoup(r.text , "lxml")
        sel = soup.select("div.card-container a")
        for s in sel:
            if '20' in s['href']:
                UrlList.append(s['href'])
                Title.append((s['href'][54:-1]))
        if(sel == []):
            break
        Page += 1
    print('[INFO]Get Title')
    print('[INFO]Get Content')
    for j in UrlList:
        r = requests.get(j, headers=headers)
        soup = BeautifulSoup(r.text, "html.parser")
        sel = soup.select("#articleBody")
        for s in sel:
            article = utils_preprocess_text(s.text, flg_stemm=False, flg_lemm=True,
                                            lst_stopwords=lst_stopwords)
            article2 = re.sub(r'http\S+|\w*\d+\w*', '', article) ##刪除網址列以及數字字串去除不必要關鍵字
            article2 = utils_preprocess_text(article2) ## 清除後再次清洗
        articleList.append(article2)
    print('[INFO]Crawl Malwarebytes Done')
    df = pd.DataFrame(data=Title, columns=['Title'])
    df['Url'] = UrlList
    hours = datetime.today().hour
    day = datetime.today().day
    month = datetime.today().month
    df['Content'] = articleList
    #df.to_excel(f'./{month}_{day}_{hours}_data.xlsx')
    return df



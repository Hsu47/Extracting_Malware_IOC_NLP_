import pandas as pd
import requests
from bs4 import BeautifulSoup
import re
import datetime
import os
import nltk
from datetime import datetime
from ArticleClean import utils_preprocess_text
from datetime import datetime


def CrawlUscert():
    print('[INFO]Crawl Uscert Start')
    ##初始化基礎變數
    Year = datetime.today().year
    Time_title = []
    Year_article = []
    temp_time = [str(i) for i in range(2004, Year + 1)]
    url = "https://us-cert.cisa.gov/ncas/alerts"
    url2 = "https://us-cert.cisa.gov/ncas/archives/alerts"
    url3 = "https://us-cert.cisa.gov"
    lst_stopwords = nltk.corpus.stopwords.words("english")
    lst_stopwords.append('network')
    lst_stopwords.append('system')
    lst_stopwords.append('systems')
    lst_stopwords.append('affected')
    lst_stopwords.append('cisa')
    lst_stopwords.append('actor')
    lst_stopwords.append('could')

    for i in temp_time:
        if (i <= '2007'):
            temp = url2 + '/' + i  ##網址
            r = requests.get(temp)
            soup = BeautifulSoup(r.text, "html.parser")
            sel = soup.select("div.view-content a")
            for s in sel:
                stixUrl = ''
                title = s.text
                temp2 = url3 + s['href']
                r = requests.get(temp2)
                soup = BeautifulSoup(r.text, "html.parser")
                sel = soup.select("article")
                for s in sel:
                    article = utils_preprocess_text(s.text, flg_stemm=False, flg_lemm=True,
                                                    lst_stopwords=lst_stopwords)
                Time_title.append([i, title, temp2, article, stixUrl])
        else:
            # for page in range(5):
            page = 0
            while(True):
                temp = url + '/' + i + '?page=' +str(page)
                r = requests.get(temp)
                soup = BeautifulSoup(r.text, "html.parser")
                sel3 =soup.select("div.view_empty")
                if(sel3): ##當沒有分頁時則去下一年
                    break
                sel = soup.select("div.view-content a")
                for s in sel:
                    stixUrl = ''
                    title = s.text
                    temp2 = url3 + s['href'] ##文章的網址
                    r = requests.get(temp2)
                    soup = BeautifulSoup(r.text, "html.parser")
                    sel1 = soup.select("article")
                    sel2 = soup.select("div.content a")
                    for s1 in sel1:  ##文章進入點
                        article2 = utils_preprocess_text(s1.text, flg_stemm=False, flg_lemm=True,
                                                     lst_stopwords=lst_stopwords)
                    for s2 in sel2:
                        if ('ioc' in s2.text.lower()):
                            if (s2['href'].startswith('/sites') and s2['href'].endswith('xml')):
                                stixUrl = url3 + s2['href']
                        elif ('stix' in s2.text.lower()):
                            if (s2['href'].startswith('/sites') and s2['href'].endswith('xml')):
                                stixUrl = url3 + s2['href']
                    Time_title.append([i, title, temp2, article2, stixUrl])
                page+=1
    df = pd.DataFrame(columns=['Year', 'Title', 'Url', 'Content', 'stixUrl'], data=Time_title)
    df = df.iloc[:, [1, 2, 3, 4]]
    print('[INFO]Crawl Uscert Done')
    df.to_excel('uscert.xlsx')
    return df